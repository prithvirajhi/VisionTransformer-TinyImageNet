# ğŸ” Vision Transformer on TinyImageNet

A pure PyTorch implementation of a Vision Transformer (ViT) trained from scratch on the TinyImageNet dataset using a single Jupyter notebook. This project documents the full training process, multiple optimization techniques, and key learnings.

---

## ğŸ“˜ Notebook

Main notebook:  
[`FunViTImplementation_TinyImageNet_30_accuracy.ipynb`](https://github.com/prithvirajhi/VisionTransformer-TinyImageNet/blob/main/FunViTImplementation_TinyImageNet_30_accuracy.ipynb)

---

## ğŸ“Š Dataset: TinyImageNet

- **200 classes**, 64Ã—64 images  
- 100,000 training images (500 per class)  
- Download: [http://cs231n.stanford.edu/tiny-imagenet-200.zip](http://cs231n.stanford.edu/tiny-imagenet-200.zip)

âš ï¸ **Important**: Before using `ImageFolder`, fix `val/` structure using `val_annotations.txt`.

---

## âš™ï¸ Model Configuration

| Hyperparameter       | Value       |
|----------------------|-------------|
| Image Size           | 64Ã—64       |
| Patch Size           | 8           |
| Embedding Dim        | 512         |
| Depth (Transformer)  | 10          |
| Attention Heads      | 8           |
| MLP Dim              | 1024        |
| Dropout              | 0.1         |
| Classes              | 200         |

---

## ğŸš€ Training Enhancements

- âš¡ **AMP (Automatic Mixed Precision)**
- ğŸ“‰ **Label Smoothing Loss**
- ğŸ”„ **Exponential Moving Average (EMA)**
- ğŸŒ€ **Cosine LR Scheduler with Warm-up**
- â¹ï¸ **Early Stopping**
- ğŸ’¾ **Checkpointing (model + EMA + optimizer + scaler)**

---

## ğŸ“ˆ Sample Results

| Epoch | Train Loss | Val Loss | Train Acc | Val Acc |
|-------|------------|----------|-----------|---------|
| 30    | 3.91       | 3.83     | 17.6%     | 18.5%   |
| 84    | 3.27       | 3.32     | 29.3%     | 27.4%   |
| 192   | 2.57       | 3.14     | 42.8%     | 32.1%   |

> â¹ï¸ Training stopped early around epoch 192 due to validation loss plateau.

---

## ğŸ§  Learnings

- Training ViTs from scratch on small datasets like TinyImageNet is possible with stabilizations.
- AMP and label smoothing improve training speed and generalization.
- EMA stabilizes evaluation and improves final accuracy.
- Cosine LR scheduling with warm-up avoids early divergence.
- Weight initialization is importantâ€”can be integrated into model class.

---

## ğŸ”§ Setup

Training and experimentation were done on:

- ğŸ–¥ï¸ **RunPod** (RTX 4000 Ada)
- ğŸ§ª **Google Colab** (for prototyping)

Ensure sufficient disk I/O performance for zip extraction and dataset operations.

---

## ğŸ§­ Future Directions

- ğŸ§ª Self-Supervised Learning (e.g., MAE, DINO)
- ğŸ“ Distillation to compact ViT or CNN
- ğŸ§Š Post-training Quantization
- ğŸ“Š Visual debugging with Grad-CAM and confusion matrix

---

> ğŸ” For suggestions or contributions, feel free to open an issue or PR.
